"""
æ¯”è¾ƒä»¥ä¸‹è®­ç»ƒå®ç°ï¼š

1. ï¼ˆéå¸¸æ…¢ï¼‰Python å‚è€ƒå®ç°
2. ä¼˜åŒ–çš„ Python å®ç°
3. HuggingFace tokenizers è®­ç»ƒå®ç°
4. æˆ‘ä»¬è‡ªå·±çš„ RustBPE è®­ç»ƒå®ç°

æ‰€æœ‰è¿™äº›éƒ½åº”è¯¥è®¡ç®—ç›¸åŒçš„åˆå¹¶å¹¶äº§ç”Ÿ
ç›¸åŒçš„è¯æ±‡è¡¨å’Œåˆ†è¯ç»“æœã€‚

æœ€åï¼Œæ¨ç†æ—¶æˆ‘ä»¬å°†ä½¿ç”¨ tiktoken ä»¥æé«˜æ•ˆç‡ã€‚
æ‰€ä»¥æˆ‘ä»¬è¦ç¡®ä¿å¯ä»¥å°†æˆ‘ä»¬çš„ rustbpe åˆ†è¯å™¨
å¯¼å‡ºåˆ° tiktoken å¹¶ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°ç›¸åŒçš„ç»“æœã€‚

è¿è¡Œæ–¹å¼ï¼š
python -m pytest tests/test_rustbpe.py -v -s
-v æ˜¯è¯¦ç»†è¾“å‡ºï¼Œ-s æ˜¯æ˜¾ç¤ºæ‰“å°
"""

import regex as re
from collections import Counter, defaultdict
import time
import rustbpe
import tiktoken
import pytest

GPT4_SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

# -----------------------------------------------------------------------------
# å‚è€ƒåˆ†è¯å™¨ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä» minbpe å¤åˆ¶ç²˜è´´å¹¶ç²¾ç®€çš„

def get_stats(ids, counts=None):
    """
    ç»™å®šä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œè¿”å›è¿ç»­å¯¹çš„è®¡æ•°å­—å…¸
    ç¤ºä¾‹ï¼š[1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    å¯é€‰åœ°å…è®¸æ›´æ–°ç°æœ‰çš„è®¡æ•°å­—å…¸
    """
    counts = {} if counts is None else counts
    for pair in zip(ids, ids[1:]): # éå†è¿ç»­å…ƒç´ 
        counts[pair] = counts.get(pair, 0) + 1
    return counts

def merge(ids, pair, idx):
    """
    åœ¨æ•´æ•°åˆ—è¡¨ï¼ˆidsï¼‰ä¸­ï¼Œå°†æ‰€æœ‰è¿ç»­å‡ºç°çš„ pair
    æ›¿æ¢ä¸ºæ–°çš„æ•´æ•° token idx
    ç¤ºä¾‹ï¼šids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    """
    newids = []
    i = 0
    while i < len(ids):
        # å¦‚æœä¸åœ¨æœ€åä¸€ä¸ªä½ç½®ä¸” pair åŒ¹é…ï¼Œåˆ™æ›¿æ¢å®ƒ
        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:
            newids.append(idx)
            i += 2
        else:
            newids.append(ids[i])
            i += 1
    return newids

class RegexTokenizer:

    def __init__(self, pattern=None):
        """
        - pattern: å¯é€‰å­—ç¬¦ä¸²ï¼Œç”¨äºè¦†ç›–é»˜è®¤å€¼ï¼ˆGPT-4 åˆ†å‰²æ¨¡å¼ï¼‰
        - special_tokens: str -> int çš„ç‰¹æ®Š token å­—å…¸
          ç¤ºä¾‹ï¼š{'<|endoftext|>': 100257}
        """
        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern
        self.merges = {} # (int, int) -> int
        self.compiled_pattern = re.compile(self.pattern)
        self.special_tokens = {}
        self.inverse_special_tokens = {}
        self.vocab = self._build_vocab()

    def _build_vocab(self):
        # vocab is simply and deterministically derived from merges
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def train(self, text, vocab_size, verbose=False):
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­åˆå¹¶æ˜¯å¦å­˜åœ¨æ­§ä¹‰ï¼ˆpair è®¡æ•°ä¸å”¯ä¸€ï¼‰
        ambiguous = False

        # split the text up into text chunks
        text_chunks = re.findall(self.compiled_pattern, text)

        # input text preprocessing
        ids = [list(ch.encode("utf-8")) for ch in text_chunks]

        # iteratively merge the most common pairs to create new tokens
        merges = {} # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes
        for i in range(num_merges):
            # è®¡ç®—æ¯ä¸ªè¿ç»­ pair å‡ºç°çš„æ¬¡æ•°
            stats = {}
            for chunk_ids in ids:
                # ä¼ å…¥ stats ä¼šåŸåœ°æ›´æ–°å®ƒï¼Œç´¯åŠ è®¡æ•°
                get_stats(chunk_ids, stats)
            # æ‰¾åˆ°è®¡æ•°æœ€é«˜çš„ pair
            pair = max(stats, key=stats.get)
            # æ£€æŸ¥åˆå¹¶æ˜¯å¦å­˜åœ¨æ­§ä¹‰ - å³æœ€å¤§å€¼ä¸å”¯ä¸€
            pair_count = stats[pair]
            pairs_with_max_count = [pair for pair, count in stats.items() if count == pair_count]
            if len(pairs_with_max_count) > 1:
                # æ‰“å°è®¡æ•°æœ€é«˜çš„å‰ 10 ä¸ª pair
                # print(f"{i} Merge is ambiguous! {pair} has {pair_count} occurrences")
                # for print_pair, print_count in sorted(stats.items(), key=lambda x: x[1], reverse=True)[:10]:
                #     print(f"{print_pair}: {print_count}")
                ambiguous = True
            # é“¸é€ æ–° tokenï¼šä¸ºå…¶åˆ†é…ä¸‹ä¸€ä¸ªå¯ç”¨çš„ id
            idx = 256 + i
            # åœ¨ ids ä¸­å°†æ‰€æœ‰ pair æ›¿æ¢ä¸º idx
            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]
            # ä¿å­˜åˆå¹¶
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]
            # æ‰“å°ä¿¡æ¯
            if verbose:
                print(f"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences")

        # save class variables
        self.merges = merges # used in encode()
        self.vocab = vocab   # used in decode()
        return ambiguous

    def _encode_chunk(self, text_bytes):
        # return the token ids
        # let's begin. first, convert all bytes to integers in range 0..255
        ids = list(text_bytes)
        while len(ids) >= 2:
            # find the pair with the lowest merge index
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            # subtle: if there are no more merges available, the key will
            # result in an inf for every single pair, and the min will be
            # just the first pair in the list, arbitrarily
            # we can detect this terminating case by a membership check
            if pair not in self.merges:
                break # nothing else can be merged anymore
            # otherwise let's merge the best pair (lowest merge index)
            idx = self.merges[pair]
            ids = merge(ids, pair, idx)
        return ids

    def encode_ordinary(self, text):
        """Encoding that ignores any special tokens."""
        # split text into chunks of text by categories defined in regex pattern
        text_chunks = re.findall(self.compiled_pattern, text)
        # all chunks of text are encoded separately, then results are joined
        ids = []
        for chunk in text_chunks:
            chunk_bytes = chunk.encode("utf-8") # raw bytes
            chunk_ids = self._encode_chunk(chunk_bytes)
            ids.extend(chunk_ids)
        return ids

# -----------------------------------------------------------------------------
# æ›´å¿«çš„ Python åˆ†è¯å™¨ï¼Œå‚è€ƒåˆ†è¯å™¨çš„ä¼˜åŒ–ç‰ˆæœ¬

def fast_merge_inplace(ids, pair, idx):
    """
    åœ¨æ•´æ•°åˆ—è¡¨ï¼ˆidsï¼‰ä¸­ï¼ŒåŸåœ°å°†æ‰€æœ‰è¿ç»­å‡ºç°çš„ pair
    æ›¿æ¢ä¸ºæ–°çš„æ•´æ•° token idx
    ç¤ºä¾‹ï¼šids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    """
    # æ‰¾åˆ° pair å‡ºç°çš„æ‰€æœ‰ä½ç½®
    i = 0
    while i < len(ids) - 1:
        if ids[i] == pair[0] and ids[i+1] == pair[1]:
            ids[i] = idx
            ids.pop(i+1)
        else:
            i += 1
    return ids


class FastRegexTokenizer:

    def __init__(self, pattern=None):
        """
        - pattern: optional string to override the default (GPT-4 split pattern)
        - special_tokens: str -> int dictionary of special tokens
          example: {'<|endoftext|>': 100257}
        """
        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern
        self.compiled_pattern = re.compile(self.pattern)
        self.special_tokens = {}
        self.inverse_special_tokens = {}
        self.merges = {}
        self.vocab = self._build_vocab()

    def _build_vocab(self):
        # vocab is simply and deterministically derived from merges
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def train(self, text, vocab_size, verbose=False):
        """
        å¼•å…¥äº†å¤šé¡¹ä¼˜åŒ–ï¼š
        - é€šè¿‡å†…è”å‡½æ•°æ¶ˆé™¤å‡½æ•°è°ƒç”¨å¼€é”€
        - ä½¿ç”¨ .pop() åŸåœ°ä¿®æ”¹ id åˆ—è¡¨è€Œä¸æ˜¯åˆ›å»ºæ–°åˆ—è¡¨
        - å°†ç›¸åŒçš„å—æŠ˜å ä¸ºå”¯ä¸€çš„å—
        - æ›´èªæ˜åœ°æ›´æ–°è®¡æ•° - åªåœ¨å—å½±å“çš„å—å‘¨å›´æ›´æ–°
        """
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # split the text up into text chunks
        text_chunks = re.findall(self.compiled_pattern, text)

        # è®¸å¤šå—æ˜¯ç›¸åŒçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥"æŠ˜å "å®ƒä»¬ä¸ºå”¯ä¸€çš„å—
        counts = Counter(text_chunks)
        unique_chunks = [ch for ch, count in counts.items()]
        chunk_counts = [count for ch, count in counts.items()]

        # è¾“å…¥æ–‡æœ¬é¢„å¤„ç†
        ids = [list(ch.encode("utf-8")) for ch in unique_chunks]
        # è¿­ä»£åˆå¹¶æœ€å¸¸è§çš„ pair ä»¥åˆ›å»ºæ–° token
        merges = {} # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes

        # åˆå§‹è®¡æ•°ï¼šæ„å»º stats å’Œä½ç½®è·Ÿè¸ª
        stats = defaultdict(int)
        positions = defaultdict(set)  # pair -> åŒ…å«æ­¤ pair çš„å—ç´¢å¼•é›†åˆ

        for chunk_idx, (chunk_ids, count) in enumerate(zip(ids, chunk_counts)):
            for pair in zip(chunk_ids, chunk_ids[1:]):
                stats[pair] += count
                positions[pair].add(chunk_idx)

        for i in range(num_merges):
            if not stats:
                break

            # æ‰¾åˆ°è®¡æ•°æœ€é«˜çš„ pair
            pair = max(stats, key=stats.get)
            # é“¸é€ æ–° tokenï¼šä¸ºå…¶åˆ†é…ä¸‹ä¸€ä¸ªå¯ç”¨çš„ id
            idx = 256 + i

            # è·å–åŒ…å«æ­¤ pair çš„å—
            affected_chunks = positions[pair]

            # è·Ÿè¸ªå¢é‡æ›´æ–°çš„è®¡æ•°å˜åŒ–
            count_changes = defaultdict(int)

            # åªåœ¨å—å½±å“çš„å—ä¸­æ›¿æ¢æ‰€æœ‰ pair å‡ºç°
            for chunk_idx in affected_chunks:
                chunk_ids = ids[chunk_idx]
                chunk_count = chunk_counts[chunk_idx]
                ix = 0
                while ix < len(chunk_ids) - 1:
                    if chunk_ids[ix] == pair[0] and chunk_ids[ix+1] == pair[1]:
                        # è·Ÿè¸ªæ­£åœ¨ç§»é™¤/æ·»åŠ çš„ pair
                        # ç§»é™¤ï¼š(prev, A), (A, B), (B, next)
                        if ix > 0:
                            old_left = (chunk_ids[ix-1], chunk_ids[ix])
                            count_changes[old_left] -= chunk_count

                        # åˆå¹¶çš„ pair æ¶ˆå¤±
                        count_changes[pair] -= chunk_count

                        if ix + 2 < len(chunk_ids):
                            old_right = (chunk_ids[ix+1], chunk_ids[ix+2])
                            count_changes[old_right] -= chunk_count

                        # åº”ç”¨åˆå¹¶
                        chunk_ids[ix] = idx
                        chunk_ids.pop(ix+1)

                        # æ·»åŠ ï¼š(prev, C), (C, next)
                        if ix > 0:
                            new_left = (chunk_ids[ix-1], chunk_ids[ix])
                            count_changes[new_left] += chunk_count

                        if ix + 1 < len(chunk_ids):
                            new_right = (chunk_ids[ix], chunk_ids[ix+1])
                            count_changes[new_right] += chunk_count
                    else:
                        ix += 1

            # å¯¹ stats å’Œ positions åº”ç”¨å¢é‡å˜åŒ–
            for changed_pair, delta in count_changes.items():
                if changed_pair == pair:
                    # åˆå¹¶çš„ pair åº”è¯¥å®Œå…¨æ¶ˆå¤±
                    continue

                stats[changed_pair] += delta

                # æ›´æ–°æ”¹å˜çš„ pair çš„ positions - åªæ£€æŸ¥å—å½±å“çš„å—
                for chunk_idx in affected_chunks:
                    chunk_ids = ids[chunk_idx]
                    contains_pair = any((chunk_ids[j], chunk_ids[j+1]) == changed_pair
                                      for j in range(len(chunk_ids) - 1))
                    if contains_pair:
                        positions[changed_pair].add(chunk_idx)
                    else:
                        positions[changed_pair].discard(chunk_idx)

            # å®Œå…¨ç§»é™¤åˆå¹¶çš„ pair
            del stats[pair]
            del positions[pair]

            # ä¿å­˜åˆå¹¶
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]

        # save class variables
        self.merges = merges # used in encode()
        self.vocab = vocab   # used in decode()

    def register_special_tokens(self, special_tokens):
        # special_tokens æ˜¯ str -> int çš„å­—å…¸
        # ç¤ºä¾‹ï¼š{"<|endoftext|>": 100257}
        self.special_tokens = special_tokens
        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}

    def decode(self, ids):
        # ç»™å®š idsï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰ï¼Œè¿”å› Python å­—ç¬¦ä¸²
        part_bytes = []
        for idx in ids:
            if idx in self.vocab:
                part_bytes.append(self.vocab[idx])
            elif idx in self.inverse_special_tokens:
                part_bytes.append(self.inverse_special_tokens[idx].encode("utf-8"))
            else:
                raise ValueError(f"invalid token id: {idx}")
        text_bytes = b"".join(part_bytes)
        text = text_bytes.decode("utf-8", errors="replace")
        return text

    def _encode_chunk(self, text_bytes):
        # return the token ids
        # let's begin. first, convert all bytes to integers in range 0..255
        ids = list(text_bytes)
        while len(ids) >= 2:
            # find the pair with the lowest merge index
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            # subtle: if there are no more merges available, the key will
            # result in an inf for every single pair, and the min will be
            # just the first pair in the list, arbitrarily
            # we can detect this terminating case by a membership check
            if pair not in self.merges:
                break # nothing else can be merged anymore
            # otherwise let's merge the best pair (lowest merge index)
            idx = self.merges[pair]
            ids = fast_merge_inplace(ids, pair, idx)
        return ids

    def encode_ordinary(self, text):
        """Encoding that ignores any special tokens."""
        # split text into chunks of text by categories defined in regex pattern
        text_chunks = re.findall(self.compiled_pattern, text)
        # all chunks of text are encoded separately, then results are joined
        ids = []
        for chunk in text_chunks:
            chunk_bytes = chunk.encode("utf-8") # raw bytes
            chunk_ids = self._encode_chunk(chunk_bytes)
            ids.extend(chunk_ids)
        return ids

# -----------------------------------------------------------------------------
# HuggingFace åˆ†è¯å™¨
from tokenizers import Tokenizer as HFTokenizer
from tokenizers import pre_tokenizers, decoders, Regex
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer

class HuggingFaceTokenizer:
    """HuggingFace Tokenizer çš„è½»é‡çº§åŒ…è£…ï¼Œæä¾›ä¸€äº›å®ç”¨åŠŸèƒ½"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    @classmethod
    def train_from_iterator(cls, text_iterator, vocab_size):
        # ä»æ–‡æœ¬è¿­ä»£å™¨è®­ç»ƒ
        # é…ç½® HuggingFace Tokenizer
        tokenizer = HFTokenizer(BPE(
            byte_fallback=True, # needed!
            unk_token=None,
            fuse_unk=False,
        ))
        # å½’ä¸€åŒ–å™¨ï¼šæ— 
        tokenizer.normalizer = None
        # é¢„åˆ†è¯å™¨ï¼šGPT-4 é£æ ¼
        gpt4_split_regex = Regex(GPT4_SPLIT_PATTERN) # huggingface è¦æ±‚ä½ ç”¨ Regex åŒ…è£…å®ƒï¼ï¼
        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior="isolated", invert=False),
            pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)
        ])
        # è§£ç å™¨ï¼šByteLevelï¼ˆä¸ ByteLevel é¢„åˆ†è¯å™¨é…å¯¹ä½¿ç”¨ï¼‰
        tokenizer.decoder = decoders.ByteLevel()
        # åå¤„ç†å™¨ï¼šæ— 
        tokenizer.post_processor = None
        # è®­ç»ƒå™¨ï¼šBPE
        trainer = BpeTrainer(
            vocab_size=vocab_size,
            show_progress=True,
            min_frequency=0, # æ— æœ€å°é¢‘ç‡
            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
            special_tokens=[], # æ— ç‰¹æ®Š token
        )
        # å¯åŠ¨è®­ç»ƒ
        tokenizer.train_from_iterator(text_iterator, trainer)
        return cls(tokenizer)

    def encode_ordinary(self, text):
        ids = self.tokenizer.encode(text, add_special_tokens=False).ids
        return ids

# -----------------------------------------------------------------------------
# æµ‹è¯•ä»¥ä¸Šæ‰€æœ‰å®ç°

@pytest.fixture(scope="module")
def enwik8_path():
    """ä¸‹è½½å¹¶ç¼“å­˜ enwik8 æ•°æ®é›†çš„ fixtureã€‚"""
    import os
    import zipfile
    from nanochat.common import get_base_dir
    base_dir = get_base_dir()
    # å°† enwik8 ä¸‹è½½å¹¶è§£å‹åˆ° .cache ç›®å½•
    enwik8_url = "https://mattmahoney.net/dc/enwik8.zip"
    enwik8_local_path = os.path.join(base_dir, "enwik8")
    enwik8_local_path_zip = os.path.join(base_dir, "enwik8.zip")
    if not os.path.exists(enwik8_local_path):
        print(f"Downloading enwik8 to {enwik8_local_path_zip}")
        import requests
        response = requests.get(enwik8_url)
        with open(enwik8_local_path_zip, "wb") as f:
            f.write(response.content)
        with zipfile.ZipFile(enwik8_local_path_zip, "r") as zip_ref:
            zip_ref.extractall(base_dir)
        print(f"Unzipped enwik8 to {enwik8_local_path}")
        os.remove(enwik8_local_path_zip)
        print(f"Removed {enwik8_local_path_zip}")
    else:
        print(f"Using existing enwik8 at {enwik8_local_path}")
    return enwik8_local_path


@pytest.fixture(scope="module")
def enwik8_small(enwik8_path):
    """æä¾› 100KB enwik8 ç”¨äºå¿«é€Ÿæµ‹è¯•çš„ fixtureã€‚"""
    with open(enwik8_path, "r", encoding="utf-8") as f:
        return f.read(100_000)

@pytest.fixture(scope="module")
def enwik8_large(enwik8_path):
    """æä¾› 10MB enwik8 ç”¨äºæ€§èƒ½æµ‹è¯•çš„ fixtureã€‚"""
    with open(enwik8_path, "r", encoding="utf-8") as f:
        return f.read(10**7)

def time_function(func, *args, **kwargs):
    """è®¡æ—¶å‡½æ•°è°ƒç”¨å¹¶è¿”å›ç»“æœå’Œè€—æ—¶"""
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    elapsed = end_time - start_time
    return result, elapsed

def test_correctness(enwik8_small):
    """æµ‹è¯•æ‰€æœ‰åˆ†è¯å™¨å®ç°äº§ç”Ÿç›¸åŒçš„ç»“æœã€‚"""
    text = enwik8_small
    encode_text = text
    vocab_size = 256 + 20  # 20 æ¬¡åˆå¹¶

    # è®­ç»ƒæ…¢é€Ÿå‚è€ƒ
    print("\nTraining slow reference...")
    slow_reference_tokenizer = RegexTokenizer()
    ambiguous_flag, slow_reference_train_time = time_function(slow_reference_tokenizer.train, text, vocab_size)
    slow_reference_ids, slow_reference_encode_time = time_function(slow_reference_tokenizer.encode_ordinary, encode_text)
    print(f"Slow reference train time: {slow_reference_train_time:.4f}s")
    print(f"Slow reference encode time: {slow_reference_encode_time:.4f}s")
    print(slow_reference_ids[:20])

    if ambiguous_flag:
        print("â€¼ï¸ WARNING: merge order was detected to be ambiguous given current text and vocab size")
        print("The implementation could be correct but we might see different results below")
    else:
        print("âœ… Merge order is NOT ambiguous")

    # è®­ç»ƒå¿«é€Ÿå‚è€ƒ
    print("\nTraining fast reference...")
    fast_reference_tokenizer = FastRegexTokenizer()
    _, fast_reference_train_time = time_function(fast_reference_tokenizer.train, text, vocab_size)
    fast_reference_ids, fast_reference_encode_time = time_function(fast_reference_tokenizer.encode_ordinary, encode_text)
    print(f"Fast reference train time: {fast_reference_train_time:.4f}s")
    print(f"Fast reference encode time: {fast_reference_encode_time:.4f}s")
    print(fast_reference_ids[:20])

    # æ–­è¨€å¿«é€Ÿç­‰äºæ…¢é€Ÿ
    assert fast_reference_ids == slow_reference_ids, "Fast reference should match slow reference"
    print("âœ… Fast == Slow")

    # Train HuggingFace
    print("\nTraining HuggingFace...")
    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)
    hf_ids, hf_encode_time = time_function(hf_tokenizer.encode_ordinary, encode_text)
    print(f"HuggingFace train time: {hf_train_time:.4f}s")
    print(f"HuggingFace encode time: {hf_encode_time:.4f}s")
    print(hf_ids[:20])

    # HuggingFace æœ‰ä¸åŒçš„å­—èŠ‚é¡ºåºï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰åŒ¹é…
    def custom_match(ids1, ids2):
        perm = {}
        for x, y in zip(ids1, ids2):
            if x < 256:
                if x in perm:
                    if perm[x] != y:
                        return False
                perm[x] = y
            if x >= 256 and x != y:
                return False
        return True

    assert custom_match(hf_ids, fast_reference_ids), "HuggingFace should match fast reference"
    print("âœ… HuggingFace == Fast")

    # æœ€åä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„ Rust å®ç°
    print("\nTraining rustbpe...")
    rustbpe_tokenizer = rustbpe.Tokenizer()
    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)
    rustbpe_ids, rustbpe_encode_time = time_function(rustbpe_tokenizer.encode, encode_text)
    print(f"RustBPE train time: {rustbpe_train_time:.4f}s")
    print(f"RustBPE encode time: {rustbpe_encode_time:.4f}s")
    print(rustbpe_ids[:20])

    assert rustbpe_ids == fast_reference_ids, "RustBPE should match fast reference"
    print("âœ… RustBPE == Fast")

    # ç°åœ¨å°† rustbpe å¯¼å‡ºåˆ° tiktoken ä»¥è¿›è¡Œæ›´é«˜æ•ˆçš„æ¨ç†
    print("\nTesting tiktoken export...")
    pattern = rustbpe_tokenizer.get_pattern()
    mergeable_ranks_list = rustbpe_tokenizer.get_mergeable_ranks()
    mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}
    enc = tiktoken.Encoding(
        name="rustbpe",
        pat_str=pattern,
        mergeable_ranks=mergeable_ranks,
        special_tokens={},
    )
    tiktoken_ids, tiktoken_encode_time = time_function(enc.encode, encode_text)
    print(f"Tiktoken encode time: {tiktoken_encode_time:.4f}s")
    print(tiktoken_ids[:20])

    assert tiktoken_ids == rustbpe_ids, "Tiktoken should match RustBPE"
    print("âœ… Tiktoken == RustBPE")


@pytest.mark.slow
def test_training_performance(enwik8_large):
    """ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†å¹¶æ¯”è¾ƒä¼˜åŒ–åˆ†è¯å™¨ï¼ˆPythonã€Rustã€HuggingFaceï¼‰çš„è®­ç»ƒé€Ÿåº¦ã€‚"""
    text = enwik8_large
    vocab_size = 2048
    print(f"\nText length: {len(text)}")

    # æ³¨é‡Šæ‰å› ä¸ºå¤ªæ…¢
    # è®­ç»ƒä¼˜åŒ–çš„ python ç‰ˆæœ¬
    # print("Training optimized python version...")
    # optimized_python_tokenizer = FastRegexTokenizer()
    # _, optimized_python_train_time = time_function(optimized_python_tokenizer.train, text, vocab_size)
    # print(f"Optimized python train time: {optimized_python_train_time:.4f}s")

    # è®­ç»ƒ rustbpe
    print("\nTraining rustbpe...")
    rustbpe_tokenizer = rustbpe.Tokenizer()
    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)
    print(f"RustBPE train time: {rustbpe_train_time:.4f}s")
    assert rustbpe_train_time > 0, "Training should take some time"

    # Train HuggingFace
    print("\nTraining HuggingFace...")
    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)
    print(f"HuggingFace train time: {hf_train_time:.4f}s")
    assert hf_train_time > 0, "Training should take some time"

    # æ‰“å°æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š Performance comparison:")
    print(f"   RustBPE: {rustbpe_train_time:.4f}s")
    print(f"   HuggingFace: {hf_train_time:.4f}s")
    print(f"   Speedup: {hf_train_time/rustbpe_train_time:.2f}x")

def test_interface(enwik8_small):
    """æµ‹è¯• RustBPETokenizer çš„è®­ç»ƒã€ç¼–ç ã€è§£ç å’Œåºåˆ—åŒ–æ¥å£ã€‚"""
    import tempfile
    from nanochat.tokenizer import RustBPETokenizer

    # ç®€å•è®­ç»ƒæµ‹è¯•
    vocab_size = 300
    tok = RustBPETokenizer.train_from_iterator([enwik8_small], vocab_size)
    assert tok.get_vocab_size() == vocab_size, f"Expected vocab size {vocab_size}, got {tok.get_vocab_size()}"
    print(f"âœ… Trained tokenizer with vocab size {vocab_size}")

    # ç¼–ç /è§£ç æ–‡æœ¬
    encode_text = "Hello world! How are you? ğŸ™ƒ"
    ids = tok.encode(encode_text)
    print(f"\nInput text: {encode_text}")
    print(f"IDs: {ids}")
    decoded = tok.decode(ids)
    print(f"Decoded: {decoded}")
    assert decoded == encode_text, f"Decoded text doesn't match: {decoded} != {encode_text}"
    print("âœ… Encode/decode test passed")

    # æ‰¹é‡ç¼–ç æµ‹è¯•
    ids_new = tok.encode([encode_text, encode_text])
    assert all(x == ids for x in ids_new), "Batch encoding should produce identical results"
    print("âœ… Encode batch OK")

    # append/prepend åŠŸèƒ½
    ids_special = tok.encode(encode_text, prepend="<|bos|>", append="<|bos|>")
    bos_token_id = tok.encode_special("<|bos|>")
    assert ids_special == [bos_token_id] + ids + [bos_token_id], "Special tokens not correctly added"
    print("âœ… append/prepend OK")

    # é€šè¿‡ä¸´æ—¶ç›®å½•ä¿å­˜/åŠ è½½æµ‹è¯•
    with tempfile.TemporaryDirectory() as tmp_dir:
        tok.save(tmp_dir)
        tok_reloaded = RustBPETokenizer.from_directory(tmp_dir)
        ids_reloaded = tok_reloaded.encode(encode_text)
        assert ids_reloaded == ids, "Reloaded tokenizer should produce same results"
        print("âœ… Save/load through temporary directory OK")
