# NanoChat 可视化流程图

本文档用 ASCII 图表展示 NanoChat 的关键流程。

---

## 🔄 完整的 LLM 训练和推理流程

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         NanoChat 完整生命周期                                 │
└─────────────────────────────────────────────────────────────────────────────┘

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                           第 1 阶段：准备阶段                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

    ┌─────────────────┐
    │  Raw Web Text   │
    │   (互联网)       │
    └────────┬────────┘
             │
             ▼
    ┌──────────────────────┐
    │  Download & Store    │
    │   (ParQuet Format)   │
    └────────┬─────────────┘
             │
             ▼
    ┌──────────────────────────────────┐
    │  Train BPE Tokenizer             │
    │  - 在 2B 字符上训练              │
    │  - 生成 65536 令牌的词表         │
    │  - 保存 tokenizer.model          │
    └────────┬─────────────────────────┘
             │
             ▼
    ┌──────────────────────────────────┐
    │  Evaluate Tokenizer              │
    │  - 计算压缩率 (BPB)              │
    │  - 生成报告                      │
    └────────┬─────────────────────────┘
             │
             ✅ 准备完成


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                          第 2 阶段：预训练 (Base)                           ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

    ┌─────────────────────────────────┐
    │  Initialize Model               │
    │  - d20 (561M parameters)        │
    │  - 20 layers, 768 embedding     │
    └────────┬────────────────────────┘
             │
             ▼
    ┌──────────────────────────────────────────────────┐
    │  Distributed Training (8 × H100)                │
    │                                                  │
    │  For step in range(num_iterations):             │
    │    ├─ Load batch from 240 data shards           │
    │    ├─ Forward pass: text → logits               │
    │    ├─ Compute loss (cross entropy)              │
    │    ├─ Backward pass (compute gradients)         │
    │    ├─ AllReduce (平均梯度)                      │
    │    ├─ Optimizer step (AdamW + Muon)            │
    │    └─ Save checkpoint every N steps             │
    │                                                  │
    │  目标: 11.2B tokens (Chinchilla)               │
    │  时间: ~1 小时 (8×H100)                        │
    └────────┬───────────────────────────────────────┘
             │
             ▼
    ┌──────────────────────────────┐
    │  Evaluate Model              │
    │  ├─ base_eval: CORE score    │
    │  └─ base_loss: BPB metric    │
    └────────┬─────────────────────┘
             │
             ▼
    ┌──────────────────────────────┐
    │  Checkpoint: base.pt         │
    │  (预训练模型)                 │
    └────────┬─────────────────────┘
             │
             ✅ 预训练完成


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                        第 3 阶段：中间训练 (Mid)                            ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

    ┌──────────────────────────┐
    │  Load base.pt            │
    └────────┬─────────────────┘
             │
             ▼
    ┌──────────────────────────────────────────────┐
    │  Multi-Task Learning (继续预训练)            │
    │                                              │
    │  Task Mixture:                               │
    │  ├─ SmolTalk (对话数据)      25%            │
    │  ├─ ARC (科学题)            20%            │
    │  ├─ MMLU (知识)             20%            │
    │  ├─ GSM8K (数学)            20%            │
    │  ├─ HumanEval (编码)        10%            │
    │  └─ SpellingBee (技能)       5%            │
    │                                              │
    │  特殊令牌学习:                              │
    │  ├─ <|user_start|>, <|user_end|>           │
    │  ├─ <|assistant_start|>, <|assistant_end|> │
    │  ├─ <|python_start|>, <|python_end|>       │
    │  └─ <|output_start|>, <|output_end|>       │
    │                                              │
    │  时间: ~40 分钟                             │
    └────────┬─────────────────────────────────────┘
             │
             ▼
    ┌──────────────────────────┐
    │  Evaluate All Tasks      │
    │  ├─ ARC                  │
    │  ├─ MMLU                 │
    │  ├─ GSM8K                │
    │  ├─ HumanEval            │
    │  └─ ChatCORE             │
    └────────┬─────────────────┘
             │
             ▼
    ┌──────────────────────────┐
    │  Checkpoint: mid.pt      │
    │  (中间训练模型)           │
    └────────┬─────────────────┘
             │
             ✅ 中间训练完成


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                      第 4 阶段：监督微调 (SFT)                             ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

    ┌──────────────────────────┐
    │  Load mid.pt             │
    └────────┬─────────────────┘
             │
             ▼
    ┌──────────────────────────────────────┐
    │  Supervised Fine-Tuning              │
    │  (质量对话数据)                      │
    │                                      │
    │  数据格式:                           │
    │  <|user_start|>问题<|user_end|>     │
    │  <|assistant_start|>回答            │
    │                      <|assistant_end|>
    │                                      │
    │  时间: ~30 分钟                     │
    └────────┬───────────────────────────┘
             │
             ▼
    ┌──────────────────────────┐
    │  Re-evaluate             │
    │  (所有任务)               │
    └────────┬─────────────────┘
             │
             ▼
    ┌──────────────────────────┐
    │  Checkpoint: sft.pt      │
    │  (最终模型)               │
    └────────┬─────────────────┘
             │
             ✅ 微调完成


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                       第 5 阶段：推理部署                                   ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

    ┌──────────────────────────┐
    │  Load sft.pt             │
    │  启用 KV 缓存             │
    └────────┬─────────────────┘
             │
             ▼
    ┌──────────────────────────────────┐
    │  Inference Engine Setup          │
    │  ├─ 加载模型到 GPU               │
    │  ├─ 初始化 KV 缓存              │
    │  └─ 准备采样参数                │
    └────────┬───────────────────────┘
             │
             ├───────────────────────────────────────┐
             │                                       │
             ▼                                       ▼
    ┌──────────────────────┐         ┌──────────────────────┐
    │  CLI Chat            │         │  Web UI Chat         │
    │  python -m           │         │  python -m           │
    │  scripts.chat_cli    │         │  scripts.chat_web    │
    │                      │         │                      │
    │  输入: "问题"        │         │  访问 WebUI 界面     │
    │  输出: AI 回答       │         │  对话和交互          │
    └──────────────────────┘         └──────────────────────┘
             │                                       │
             └───────────────────────────────────────┘
                       ▼
             ✅ 推理部署完成

```

---

## 🧠 模型架构内部结构

```
┌─────────────────────────────────────────────────────────┐
│                    输入: token_ids                       │
│                    shape: [B, T]                        │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
        ┌────────────────────────────┐
        │    Token Embedding         │
        │    wte: vocab → d_embd     │
        │    [B, T] → [B, T, 768]    │
        └────────────┬───────────────┘
                     │
                     ▼
        ┌────────────────────────────┐
        │    Norm + Embedding        │
        │    RMSNorm (no params)     │
        └────────────┬───────────────┘
                     │
         ╔═══════════╩═══════════╗
         ║   Transformer Block   ║
         ║      (×20 layers)     ║
         ║  each layer has:      ║
         ║  - Self Attention     ║
         ║  - Feed Forward       ║
         ║  - Residual Connect   ║
         ║  - Layer Norm         ║
         ╚═════════╤═════════════╝
                   │
        ┌──────────▼───────────┐
        │                      │
        │  Self Attention      │
        │  ┌──────────────┐    │
        │  │ Q, K, V Proj │    │
        │  └──┬────┬────┬─┘    │
        │     │    │    │      │
        │     ▼    ▼    ▼      │
        │  ┌──────────────┐    │
        │  │ Rotary Emb   │    │
        │  │ (RoPE)       │    │
        │  └──┬────┬──────┘    │
        │     │    │           │
        │     ▼    ▼           │
        │  ┌──────────────┐    │
        │  │ QK Norm      │    │
        │  └──┬────┬──────┘    │
        │     │    │           │
        │     ▼    ▼           │
        │  ┌─────────────────┐ │
        │  │ Scaled Dot      │ │
        │  │ Product Attn    │ │
        │  │ + GQA Support   │ │
        │  │ + KV Cache      │ │
        │  └────────┬────────┘ │
        │           │          │
        │           ▼          │
        │  ┌──────────────┐    │
        │  │ Output Proj  │    │
        │  └──────┬───────┘    │
        │         │            │
        └─────────┼────────────┘
                  │
        ┌─────────▼──────────┐
        │  Residual Add      │
        │  x = x + attn_out  │
        └─────────┬──────────┘
                  │
        ┌─────────▼────────────┐
        │  Feed Forward (FFN)  │
        │  Linear (d → 4d)     │
        │  ReLU² activation    │
        │  Linear (4d → d)     │
        └─────────┬────────────┘
                  │
        ┌─────────▼──────────────┐
        │  Residual Add          │
        │  x = x + ffn_out       │
        └─────────┬──────────────┘
                  │
             (Loop ×20)
                  │
                  ▼
        ┌────────────────────────┐
        │  Final RMSNorm         │
        │  (no learned params)   │
        └────────────┬───────────┘
                     │
                     ▼
        ┌────────────────────────────┐
        │  Output Projection         │
        │  (LM Head)                 │
        │  [B, T, 768] →             │
        │  [B, T, vocab_size=50304]  │
        └────────────┬───────────────┘
                     │
                     ▼
        ┌────────────────────────────┐
        │  Logits: [B, T, 50304]     │
        │  用于训练: cross_entropy   │
        │  用于推理: sample/argmax   │
        └────────────────────────────┘

图例:
[B, T, D] = shape (Batch, Time/Seq, Dimension)
→ = tensor 操作
↓ = forward pass
```

---

## 📊 数据加载流程 (DDP)

```
┌──────────────────────────────────────────────────────────┐
│            8 个 GPU 分布式数据加载                        │
└──────────────────────────────────────────────────────────┘

GPU_0                  GPU_1                   ... GPU_7
  │                      │                        │
  ▼                      ▼                        ▼
Shard 0              Shard 1                  Shard 7
(250M chars)         (250M chars)             (250M chars)
  │                      │                        │
  ▼                      ▼                        ▼
┌──────────────┐   ┌──────────────┐        ┌──────────────┐
│ Parquet      │   │ Parquet      │        │ Parquet      │
│ Files        │   │ Files        │        │ Files        │
├──────────────┤   ├──────────────┤        ├──────────────┤
│ Row Group 0  │   │ Row Group 8  │        │ Row Group 56 │
│ Row Group 8  │   │ Row Group 16 │        │ Row Group 64 │
│ ... (×DDP)   │   │ ... (×DDP)   │        │ ... (×DDP)   │
└──────┬───────┘   └──────┬───────┘        └──────┬───────┘
       │                  │                       │
       ▼                  ▼                       ▼
┌──────────────┐   ┌──────────────┐        ┌──────────────┐
│ Text Lines   │   │ Text Lines   │        │ Text Lines   │
│ (batch)      │   │ (batch)      │        │ (batch)      │
└──────┬───────┘   └──────┬───────┘        └──────┬───────┘
       │                  │                       │
       ▼                  ▼                       ▼
┌──────────────┐   ┌──────────────┐        ┌──────────────┐
│ Tokenizer    │   │ Tokenizer    │        │ Tokenizer    │
│ (divide work)│   │ (divide work)│        │ (divide work)│
└──────┬───────┘   └──────┬───────┘        └──────┬───────┘
       │                  │                       │
       ▼                  ▼                       ▼
┌──────────────┐   ┌──────────────┐        ┌──────────────┐
│ Token IDs    │   │ Token IDs    │        │ Token IDs    │
│ [N tokens]   │   │ [N tokens]   │        │ [N tokens]   │
└──────┬───────┘   └──────┬───────┘        └──────┬───────┘
       │                  │                       │
       ▼                  ▼                       ▼
┌──────────────────────────────────────────┐
│ Concatenate & Batch Formation            │
│ [Device 0 tokens][Device 1 tokens]..     │
│ → [B×seq_len tokens across all GPUs]     │
└──────┬───────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────┐
│ Reshape: [B, T]                          │
│ B = 32 (per device)                      │
│ T = 2048 (sequence length)               │
└──────┬───────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────┐
│ Model Forward Pass (同时在 8 GPU 上)    │
│ Logits: [B, T, vocab_size]               │
└──────┬───────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────┐
│ Compute Loss (每个 GPU 各自计算)        │
│ loss = cross_entropy(logits, targets)    │
└──────┬───────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────┐
│ Backward Pass (每个 GPU 各自计算)       │
│ Compute gradients locally                 │
└──────┬───────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────┐
│ AllReduce (通信)                         │
│ 同步梯度到所有 GPU                       │
│ NCCL 通信，GPU 直连                      │
└──────┬───────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────┐
│ Optimizer Step (所有 GPU 同步)          │
│ 所有 GPU 使用相同梯度更新                │
└──────────────────────────────────────────┘
```

---

## 🚀 推理过程 (KV 缓存)

```
没有 KV 缓存的推理 (训练时):
═════════════════════════════════════

输入序列: [1, 2, 3, 4]

第 1 步:
  Q_all = [1, 2, 3, 4] → Q embeddings  [4, 768]
  K_all = [1, 2, 3, 4] → K embeddings  [4, 768]
  V_all = [1, 2, 3, 4] → V embeddings  [4, 768]
  
  Attention = softmax((Q_all @ K_all^T) / √d_k) @ V_all
  输出: [4, 768]

总计算复杂度: O(4²) = O(16)


有 KV 缓存的推理 (生成时):
════════════════════════════

初始输入: [1, 2, 3]
缓存: 无

第 1 次前向传播 (prefill):
  ┌─────────────┐
  │ Input: [1]  │
  └────┬────────┘
       │
       ▼
  ┌─────────────────────┐
  │ Q_new = token1 emb  │ [1, 768]
  │ K_new = token1 emb  │ [1, 768]
  │ V_new = token1 emb  │ [1, 768]
  └────┬────────────────┘
       │
       ▼
  ┌──────────────────┐
  │ Attention Output │ [1, 768]
  └────┬─────────────┘
       │
       ▼
  ┌──────────────────────────┐
  │ Save to KV Cache:        │
  │ cache.K = [1, 768]       │
  │ cache.V = [1, 768]       │
  └────┬─────────────────────┘
       │
       ▼
  ┌──────────────────────────┐
  │ Logits → Sample Token 4   │
  └──────────────────────────┘

计算: O(1²) = 1

第 2 次前向传播 (decode，使用缓存):
  ┌──────────────────┐
  │ Input: [token_4] │ (只有新令牌！)
  └────┬─────────────┘
       │
       ▼
  ┌──────────────────────┐
  │ Q_new = token4 emb   │ [1, 768]
  │ K_new = token4 emb   │ [1, 768]
  │ V_new = token4 emb   │ [1, 768]
  └────┬─────────────────┘
       │
       ▼
  ┌─────────────────────────────────────┐
  │ 从缓存读取:                         │
  │ K_all = concat(cache.K, K_new)     │ [2, 768]
  │ V_all = concat(cache.V, V_new)     │ [2, 768]
  └────┬────────────────────────────────┘
       │
       ▼
  ┌───────────────────────────────┐
  │ Attention(Q_new, K_all, V_all)│ [1, 768]
  │ 注意 Q_new 只有 1 个令牌！      │
  │ 与所有 K_all (2 个) 点积       │
  └────┬────────────────────────────┘
       │
       ▼
  ┌──────────────────────────────┐
  │ 更新 KV 缓存:                │
  │ cache.K = K_all  [2, 768]   │
  │ cache.V = V_all  [2, 768]   │
  └────┬─────────────────────────┘
       │
       ▼
  ┌──────────────────────────────┐
  │ Logits → Sample Token 5      │
  └──────────────────────────────┘

计算: Q_new (1) @ K_all (2) = O(1×2) = 2

对比:
─────────────────────────────────────────
每步计算       | 无缓存  | 有缓存      | 提速
─────────────────────────────────────────
Step 1        | O(1²)  | O(1²)      | 1×
Step 2        | O(2²)  | O(1×2)     | 2×
Step 3        | O(3²)  | O(1×3)     | 3×
...
Step 100      | O(100²)| O(1×100)   | 100×
─────────────────────────────────────────

总计算复杂度:
  无缓存: 1² + 2² + 3² + ... + 100² = O(n³/3) = 333,350
  有缓存: 1 + 2 + 3 + ... + 100   = O(n²/2) = 5,050
  
  提速: 333,350 / 5,050 ≈ 66 倍!

实际上对于长序列 (2048):
  无缓存: ~2.8 billion ops
  有缓存: ~2.1 million ops
  
  提速: ~1000 倍!
```

---

## 🔄 训练循环详解

```
┌─────────────────────────────────────────────────────────┐
│                 单个训练步骤的流程                       │
└─────────────────────────────────────────────────────────┘

step 1: 加载数据
┌────────────────────────────────────┐
│ batch = dataloader.get_batch()      │
│ Input:  [B, T] token IDs           │
│ Target: [B, T] target token IDs    │
│         (shifted by 1)              │
└────────────┬───────────────────────┘
             │
step 2: 前向传播
┌────────────────────────────────────┐
│ logits, _ = model(batch['input'])  │
│ logits shape: [B, T, vocab_size]   │
└────────────┬───────────────────────┘
             │
step 3: 计算损失
┌────────────────────────────────────┐
│ loss = F.cross_entropy(            │
│   logits.view(-1, vocab_size),     │
│   batch['target'].view(-1)         │
│ )                                  │
│ loss: scalar                       │
└────────────┬───────────────────────┘
             │
step 4: 反向传播
┌────────────────────────────────────┐
│ loss.backward()                    │
│ 计算 ∂loss/∂param 对所有参数      │
└────────────┬───────────────────────┘
             │
step 5: DDP AllReduce (分布式)
┌────────────────────────────────────┐
│ dist.all_reduce(grad)              │
│ 同步所有 GPU 的梯度                │
│ 结果: 所有 GPU 的梯度相同           │
└────────────┬───────────────────────┘
             │
step 6: 梯度裁剪 (可选)
┌────────────────────────────────────┐
│ torch.nn.utils.clip_grad_norm_(    │
│   model.parameters(),              │
│   grad_clip=1.0                    │
│ )                                  │
│ 防止梯度爆炸                        │
└────────────┬───────────────────────┘
             │
step 7: 优化器步骤
┌────────────────────────────────────────────┐
│ optimizer.step()                           │
│                                            │
│ 对于 Embedding/Unembedding 参数:         │
│   用 AdamW 更新                            │
│   param = param - lr × (m + λ×param)      │
│   其中 m = 动量估计                        │
│                                            │
│ 对于矩阵参数:                              │
│   用 Muon (Orthogonal SGD) 更新           │
│   param = param - lr × g                  │
│   g = 正交化后的梯度                       │
└────────────┬───────────────────────────────┘
             │
step 8: 清空梯度
┌────────────────────────────────────┐
│ optimizer.zero_grad()              │
│ 清空梯度缓冲，准备下一步           │
└────────────┬───────────────────────┘
             │
step 9: 日志记录
┌────────────────────────────────────┐
│ if step % log_every == 0:          │
│   wandb.log({                      │
│     'loss': loss.item(),           │
│     'step': step,                  │
│     'lr': current_lr,              │
│   })                               │
│   print(f"Step {step}: {loss}")    │
└────────────┬───────────────────────┘
             │
step 10: 检查点保存
┌────────────────────────────────────┐
│ if step % save_every == 0:         │
│   checkpoint = {                   │
│     'model': model.state_dict(),   │
│     'optimizer': opt.state_dict(), │
│     'step': step,                  │
│   }                                │
│   torch.save(checkpoint, path)     │
└────────────┬───────────────────────┘
             │
             ▼
        下一步迭代
        
时间分解:
─────────────────────────────────────────────
Forward:        ~100ms
Backward:       ~100ms
AllReduce:      ~50ms (通信)
Optimizer:      ~20ms
Log/Save:       ~5ms (除第一个检查点)
─────────────────────────────────────────────
总计:          ~275ms/step

对于预训练:
  步数: ~21,000 (11.2B tokens / 524K tokens/step)
  总时间: 21,000 × 0.275s ≈ 1.6 小时 ✓
```

---

## 🎯 模型大小与性能关系

```
┌─────────────────────────────────────────────────────────┐
│           参数数 vs 训练时间 vs 推理速度                 │
└─────────────────────────────────────────────────────────┘

参数数 (M) │ 层数 │ 训练令牌 | 训练时间 | 推理速度 | 显存
───────────┼──────┼─────────┼─────────┼──────────┼──────
561        │ d20  │ 11.2B   │ 1h      │ 400 t/s  │ 40GB
1300       │ d26  │ 26B     │ 3.5h    │ 250 t/s  │ 50GB ⚠️
1900       │ d32  │ 38B     │ 6h      │ 200 t/s  │ 65GB ⚠️

图表:

训练时间 vs 参数
    ▲
  7 │                         ● (d32)
    │
  6 │
    │
  5 │                    ● (d26)
    │
  4 │
    │
  3 │
    │
  2 │              ● (d20)
    │
  1 │        ●
    │
  0 └────────┴────────┴────────┴────────► 参数数
    0        500      1000     1500  2000

推理吞吐 vs 参数
    ▲
500 │ ● (d20)
    │
400 │
    │
300 │
    │      ● (d26)
200 │
    │      
100 │                    ● (d32)
    │
  0 └────────┴────────┴────────┴────────► 参数数
    0        500      1000     1500  2000

显存 vs 参数
    ▲
80  │                         ● (d32, 接近极限)
    │
70  │
    │                    ● (d26, 警告)
60  │
    │
50  │
    │              ● (d20, 舒适)
40  │
    │
30  │
    │
20  │
    │
10  │
    │
  0 └────────┴────────┴────────┴────────► 参数数
    0        500      1000     1500  2000

NanoChat 推荐配置:
┌─────────────────────────────────────────────┐
│ 对于 8×H100 (80GB 显存):                     │
│ ├─ 推荐: d20 (561M) - 最平衡                │
│ ├─ 可选: d26 (1.3B) - 略显吃紧              │
│ └─ 不推荐: d32 (1.9B) - 容易 OOM           │
│                                             │
│ 对于 8×A100-40GB (40GB 显存):              │
│ ├─ 推荐: d12 (200M) - 安全                 │
│ ├─ 可选: d16 (350M) - 紧凑               │
│ └─ 不推荐: d20+ - 容易 OOM                │
└─────────────────────────────────────────────┘
```

---

## 🎯 总结

通过这些可视化流程图，你可以理解：

✅ **完整生命周期**: 从数据准备到部署的每个阶段  
✅ **模型架构**: Transformer 内部的计算图  
✅ **分布式训练**: 如何在多个 GPU 上并行工作  
✅ **推理优化**: KV 缓存如何 100 倍加速  
✅ **性能权衡**: 参数数、训练时间、推理速度的关系  

下一步：

1. 阅读 [PROJECT_MAP_CN.md](PROJECT_MAP_CN.md) 了解详细说明
2. 参考 [QUICK_REFERENCE_CN.md](QUICK_REFERENCE_CN.md) 运行命令
3. 研究 [ADVANCED_PRINCIPLES_CN.md](ADVANCED_PRINCIPLES_CN.md) 理解原理

---

祝你学习愉快！🚀
