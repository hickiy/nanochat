# NanoChat å¿«é€Ÿå‚è€ƒæŒ‡å—

## ğŸ“Œ æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥è¡¨

### 1. æ¨¡å‹è§„æ ¼é€ŸæŸ¥

```
æ¨¡å‹è§„æ ¼è¡¨:

å±æ€§              | d20 ($100)    | d26 ($300)    | d32 ($1000)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å‚æ•°æ•°é‡          | 561M          | 1.3B          | 1.9B
å±‚æ•° (depth)      | 20            | 26            | 32
æ³¨æ„åŠ›å¤´æ•°        | 6             | 8             | 8
KV å¤´æ•° (GQA)     | 6             | 8             | 8
åµŒå…¥ç»´åº¦          | 768           | 1024          | 1024
ä¸Šä¸‹æ–‡é•¿åº¦        | 2048          | 2048          | 2048
è®­ç»ƒä»¤ç‰Œæ•°        | 11.2B         | 26B           | 38B
é¢„è®¡è®­ç»ƒæ—¶é—´      | ~4 å°æ—¶       | ~12 å°æ—¶      | ~41.6 å°æ—¶
GPU é…ç½®          | 8Ã—H100        | 8Ã—H100        | 8Ã—H100
é¢„è®¡æˆæœ¬          | $100          | $300          | $1000

è®¡ç®—è¯´æ˜:
- Chinchilla å®šå¾‹: tokens = 20 Ã— params
- tokens = 2.5B å‚æ•° Ã— 20 = 11.2B
- åˆ†è¯æ¯”ç‡: 4.8 å­—ç¬¦/ä»¤ç‰Œ
- å­—ç¬¦æ•° = 11.2B Ã— 4.8 = 53.7B å­—ç¬¦
```

### 2. å…³é”®å‚æ•°é…ç½®

```python
# nanochat/gpt.py ä¸­çš„é»˜è®¤é…ç½®

GPTConfig é»˜è®¤å€¼:
â”œâ”€ sequence_len = 1024          # æœ€å¤§åºåˆ—é•¿åº¦
â”œâ”€ vocab_size = 50304           # è¯æ±‡è¡¨å¤§å° (65536 for tok)
â”œâ”€ n_layer = 12                 # Transformer å±‚æ•°
â”œâ”€ n_head = 6                   # æŸ¥è¯¢å¤´æ•°
â”œâ”€ n_kv_head = 6                # é”®å€¼å¤´æ•°
â””â”€ n_embd = 768                 # åµŒå…¥ç»´åº¦

# scripts/base_train.py ä¸­çš„ä¼˜åŒ–å‚æ•°

device_batch_size = 32          # å• GPU ä¸Šçš„åºåˆ—æ•°
total_batch_size = 524288       # å…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆä»¤ç‰Œæ•°ï¼‰
max_seq_len = 2048              # æœ€å¤§åºåˆ—é•¿åº¦

å­¦ä¹ ç‡:
â”œâ”€ embedding_lr = 0.2           # Embedding å‚æ•° (AdamW)
â”œâ”€ matrix_lr = 0.02             # çŸ©é˜µå‚æ•° (Muon)
â”œâ”€ unembedding_lr = 0.004       # Output projection (AdamW)
â””â”€ init_lr_frac = 1.0           # åˆå§‹å­¦ä¹ ç‡æ¯”ä¾‹

æ­£åˆ™åŒ–:
â”œâ”€ grad_clip = 1.0              # æ¢¯åº¦è£å‰ª
â”œâ”€ weight_decay = 0.0           # AdamW æƒé‡è¡°å‡
â””â”€ dropout = 0.0                # Dropout (æœªä½¿ç”¨)

target_param_data_ratio = 20    # Chinchilla å®šå¾‹
```

### 3. æ–‡ä»¶æ‰§è¡Œæ—¶é—´å‚è€ƒ

```
åœ¨ 8Ã—H100 èŠ‚ç‚¹ä¸Šæ‰§è¡Œæ—¶é—´ (speedrun.sh ä¸­):

é˜¶æ®µ                          | æ—¶é—´    | ç´¯è®¡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä¾èµ–å®‰è£… & ç¯å¢ƒè®¾ç½®           | 10 åˆ†é’Ÿ | 0:10
åˆ†è¯å™¨è®­ç»ƒ                    | 15 åˆ†é’Ÿ | 0:25
æ•°æ®ä¸‹è½½ (240 ä¸ªåˆ†ç‰‡)         | 30 åˆ†é’Ÿ | 0:55 (åå°)
é¢„è®­ç»ƒ (base_train)           | 60 åˆ†é’Ÿ | 2:00
é¢„è®­ç»ƒè¯„ä¼° (base_eval/loss)   | 20 åˆ†é’Ÿ | 2:20
ä¸­é—´è®­ç»ƒ (mid_train)          | 40 åˆ†é’Ÿ | 3:00
ç›‘ç£å¾®è°ƒ (chat_sft)           | 30 åˆ†é’Ÿ | 3:30
æœ€ç»ˆè¯„ä¼°                      | 10 åˆ†é’Ÿ | 3:40
æŠ¥å‘Šç”Ÿæˆ                      | 5 åˆ†é’Ÿ  | 3:45

æ€»æ—¶é—´: ~4 å°æ—¶

æ€§èƒ½æŒ‡æ ‡ (d20, 8Ã—H100):
- ä»¤ç‰Œååé‡: ~200K tokens/sec
- GPU åˆ©ç”¨ç‡: 85-90%
- æ˜¾å­˜å ç”¨: ~40 GB (æ€»)/GPU
- é€šä¿¡å¼€é”€: ~15%
```

### 4. æ•°æ®æµè·¯å¾„

```
åŸå§‹æ–‡æœ¬æ•°æ®æµ:
  Raw Text (äº’è”ç½‘) 
    â†“ [Dataset.download()]
  .jsonl æ–‡ä»¶ (~250MB/æ–‡ä»¶)
    â†“ [Parquet è½¬æ¢]
  .parquet æ–‡ä»¶ (è¡Œç»„ä¼˜åŒ–)
    â†“ [DDP åˆ†ç‰‡è¯»å–]
  Text å­—ç¬¦ä¸² (batch)
    â†“ [åˆ†è¯å™¨]
  Token IDs [B, T]
    â†“ [æ¨¡å‹]
  Logits [B, T, vocab_size]
    â†“ [æŸå¤±è®¡ç®—]
  Loss (æ ‡é‡)
    â†“ [åå‘ä¼ æ’­]
  Gradients
    â†“ [AllReduce (DDP)]
  Averaged Gradients
    â†“ [Optimizer Step]
  Updated Weights âœ“
```

### 5. ä»»åŠ¡ç±»å‹é€ŸæŸ¥

```python
# tasks/ ä¸­å¯ç”¨çš„ä»»åŠ¡

å¯¹è¯ç±»:
â”œâ”€ SmolTalk()                # ä¸€èˆ¬å¯¹è¯æ•°æ®
â”œâ”€ CustomJSON()              # è‡ªå®šä¹‰ JSON æ ¼å¼
â””â”€ <|ç‰¹æ®Šä»¤ç‰Œ|>             # å¯¹è¯æ ‡è®°

è¯„ä¼°ç±» (å¤šé¡¹é€‰æ‹©):
â”œâ”€ ARC()                     # ARC Challenge (Science)
â”œâ”€ MMLU()                    # å¤šé¢†åŸŸçŸ¥è¯†
â””â”€ å‡†ç¡®ç‡è¯„ä¼°

è®¡ç®—ç±»:
â”œâ”€ GSM8K()                   # å°å­¦æ•°å­¦
â””â”€ HumanEval()               # Python ç¼–ç 

æŠ€èƒ½åŸ¹å…»:
â””â”€ SpellingBee()             # æ‹¼å†™/è®¡æ•°ä»»åŠ¡

é›†åˆ:
â”œâ”€ TaskMixture()             # éšæœºæ··åˆå¤šä¸ªä»»åŠ¡
â””â”€ TaskSequence()            # é¡ºåºæ‰§è¡Œä»»åŠ¡
```

---

## ğŸš€ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

### å®‰è£…ä¸ç¯å¢ƒ

```bash
# å®‰è£… Rust (ç”¨äº BPE)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source "$HOME/.cargo/env"

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv venv
source .venv/bin/activate

# å®‰è£…ä¾èµ–
uv sync --extra gpu
```

### è®­ç»ƒå‘½ä»¤

```bash
# å• GPU é¢„è®­ç»ƒ (å°æ¨¡å‹æµ‹è¯•)
python -m scripts.base_train \
  --depth=12 \
  --device_batch_size=8 \
  --max_seq_len=1024 \
  --num_iterations=100

# å¤š GPU é¢„è®­ç»ƒ (å®Œæ•´è®­ç»ƒ)
torchrun --standalone --nproc_per_node=8 \
  -m scripts.base_train \
  --depth=20 \
  --run=my_experiment

# ä¸­é—´è®­ç»ƒ (å¤šä»»åŠ¡å­¦ä¹ )
torchrun --standalone --nproc_per_node=8 \
  -m scripts.mid_train \
  --device_batch_size=32

# ç›‘ç£å¾®è°ƒ
torchrun --standalone --nproc_per_node=8 \
  -m scripts.chat_sft \
  --num_iterations=500

# å¼ºåŒ–å­¦ä¹  (å¯é€‰)
torchrun --standalone --nproc_per_node=8 \
  -m scripts.chat_rl \
  --num_iterations=200
```

### è¯„ä¼°å‘½ä»¤

```bash
# è¯„ä¼° CORE å¾—åˆ† (åŸºç¡€èƒ½åŠ›)
torchrun --standalone --nproc_per_node=8 \
  -m scripts.base_eval

# è¯„ä¼°å¤šä¸ªä»»åŠ¡
torchrun --standalone --nproc_per_node=8 \
  -m scripts.chat_eval \
  --model_tag=sft \
  --all_tasks

# åªè¯„ä¼°ç‰¹å®šä»»åŠ¡
python -m scripts.chat_eval \
  --model_tag=sft \
  -a GSM8K      # åªè¯„ä¼°æ•°å­¦
```

### äº¤äº’å‘½ä»¤

```bash
# CLI èŠå¤©
python -m scripts.chat_cli \
  --model_tag=sft

# å¸¦å‰ç¼€çš„ CLI (è‡ªåŠ¨å›å¤)
python -m scripts.chat_cli \
  --model_tag=sft \
  -p "Hello, my name is"

# Web UI (æ¨èæ–¹å¼)
python -m scripts.chat_web \
  --model_tag=sft \
  --port=8000

# è®¿é—® UI
open http://localhost:8000
# æˆ– http://<public_ip>:8000 (è¿œç¨‹)
```

### æ•°æ®ä¸åˆ†è¯

```bash
# ä¸‹è½½æ•°æ®åˆ†ç‰‡
python -m nanochat.dataset -n 240

# è®­ç»ƒåˆ†è¯å™¨
python -m scripts.tok_train \
  --max_chars=2000000000 \
  --vocab_size=65536

# è¯„ä¼°åˆ†è¯å™¨
python -m scripts.tok_eval

# åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†
python -c "
from tasks.customjson import CustomJSON
task = CustomJSON('my_data.jsonl')
batch = task.get_batch()
print(batch)
"
```

### Weights & Biases (W&B) é›†æˆ

```bash
# ç™»å½• W&B
wandb login

# å¸¦ W&B æ—¥å¿—çš„è®­ç»ƒ
WANDB_RUN=my_experiment bash speedrun.sh

# ç¦ç”¨ W&B (é»˜è®¤)
WANDB_RUN=dummy bash speedrun.sh

# æŸ¥çœ‹æ—¥å¿—
wandb sync
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### æ£€æŸ¥ GPU çŠ¶æ€

```bash
# å®æ—¶ç›‘æ§ GPU
watch -n 1 nvidia-smi

# è¯¦ç»† GPU ä¿¡æ¯
nvidia-smi -q

# ç‰¹å®šè¿›ç¨‹çš„æ˜¾å­˜ä½¿ç”¨
nvidia-smi pmon

# GPU åŠŸè€—
nvidia-smi --query-gpu=power.draw,power.limit \
  --format=csv,noheader
```

### æ€§èƒ½åˆ†æ

```python
# åœ¨è„šæœ¬ä¸­æ·»åŠ æ€§èƒ½åˆ†æ
import torch.profiler as profiler

with profiler.profile(
    activities=[profiler.ProfilerActivity.CPU, 
                profiler.ProfilerActivity.CUDA],
    on_trace_ready=profiler.tensorboard_trace_handler('./logs'),
    record_shapes=True,
) as prof:
    for step in range(100):
        loss = model(batch)
        loss.backward()
        optimizer.step()
    
# æŸ¥çœ‹ TensorBoard
tensorboard --logdir ./logs
```

### å¸¸è§é”™è¯¯åŠè§£å†³

```
é”™è¯¯ 1: CUDA Out of Memory
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RuntimeError: CUDA out of memory

è§£å†³:
1. å‡å°‘ device_batch_size: 32 â†’ 16 â†’ 8 â†’ 4
2. å‡å°‘ max_seq_len: 2048 â†’ 1024 â†’ 512
3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: --grad_checkpoint=1
4. æ··åˆç²¾åº¦: --dtype=float16


é”™è¯¯ 2: åˆ†å¸ƒå¼è®­ç»ƒä¸å·¥ä½œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RuntimeError: No resource available for device

è§£å†³:
1. æ£€æŸ¥ NVIDIA NCCL: nvidia-smi topo -m
2. æ£€æŸ¥ GPU äº’è¿: nvidia-smi -q | grep Link
3. ä½¿ç”¨ NCCL_DEBUG: NCCL_DEBUG=INFO torchrun ...


é”™è¯¯ 3: æ•°æ®åŠ è½½ç¼“æ…¢
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Training speed å¾ˆæ…¢ï¼ŒGPU åˆ©ç”¨ç‡ä½

è§£å†³:
1. å¢åŠ  num_workers: dataloader(num_workers=4)
2. é¢„åŠ è½½æ•°æ®: python -m nanochat.dataset -n 300
3. ä½¿ç”¨æœ¬åœ° SSD: cp data.parquet /ssd/


é”™è¯¯ 4: æ¨¡å‹ä¸æ”¶æ•›
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Loss ä¸ä¸‹é™æˆ–åœ¨æŒ¯è¡

è§£å†³:
1. å‡å°å­¦ä¹ ç‡: matrix_lr=0.01 (from 0.02)
2. å¢åŠ  grad_clip: grad_clip=0.5 (from 1.0)
3. æ£€æŸ¥æ•°æ®è´¨é‡: æ‰‹åŠ¨æ£€æŸ¥æ ·æœ¬
4. ä»æ£€æŸ¥ç‚¹æ¢å¤: å‡å°‘å­¦ä¹ ç‡ 50%
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### è®­ç»ƒæ€§èƒ½

```
ç¡¬ä»¶          | æ¨¡å‹    | Tokens/sec | GPU Util | æ˜¾å­˜ç”¨
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2Ã—A100-40GB   | d12     | 50K        | 75%      | 35GB
2Ã—A100-80GB   | d20     | 100K       | 80%      | 60GB
8Ã—H100        | d20     | 200K       | 85%      | 40GB
8Ã—H100        | d26     | 120K       | 75%      | 50GB (OOM å¯èƒ½)
8Ã—H100        | d32     | 80K        | 65%      | 65GB (OOM å¯èƒ½)
å• A100-40GB  | d12     | 6K         | 50%      | 35GB (æ¢¯åº¦ç´¯ç§¯)
å• H100       | d12     | 15K        | 60%      | 40GB (æ¢¯åº¦ç´¯ç§¯)
```

### æ¨ç†æ€§èƒ½

```
é…ç½®              | ååé‡        | å»¶è¿Ÿï¼ˆé¦–tokenï¼‰| æ˜¾å­˜ç”¨
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
H100, æ—  cache    | 100 tok/s     | 10ms           | 5GB
H100, æœ‰ cache    | 400 tok/s     | 2.5ms/token    | 5GB
H100, batch=16    | 3000 tok/s    | 5ms (å¹³å‡)     | 8GB
2Ã—A100, batch=8   | 800 tok/s     | 10ms (å¹³å‡)    | 8GB
CPU (8æ ¸)         | 0.1 tok/s     | 10000ms        | 15GB (æ…¢)
MPS (M1)          | 1 tok/s       | 1000ms         | 8GB  (æ…¢)
```

### åˆ†è¯æ€§èƒ½

```
æ“ä½œ              | é€Ÿåº¦          | è¯´æ˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åˆ†è¯è®­ç»ƒ (RustBPE) | 500M chars/s  | å®Œæ•´ 2B å­—ç¬¦åªéœ€ 4 ç§’
åˆ†è¯ç¼–ç  (tiktoken)| 10M tokens/s  | ä½¿ç”¨ GPU åŠ é€Ÿ
åˆ†è¯è§£ç  (tiktoken)| 10M tokens/s  | éå¸¸å¿«
```

---

## ğŸ“ˆ æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°è¯„ä¼°ä»»åŠ¡

```python
# åˆ›å»º tasks/mytask.py

from tasks.common import Task

class MyTask(Task):
    def __init__(self):
        self.data = []  # åŠ è½½ä½ çš„æ•°æ®
    
    def get_batch(self):
        """è¿”å› (prompt, completion) å…ƒç»„åˆ—è¡¨"""
        return [
            ("Q: What is 2+2?\nA:", " 4"),
            ("Q: What is 3+3?\nA:", " 6"),
        ]
    
    def evaluate(self, completions):
        """è¯„ä¼°æ¨¡å‹è¾“å‡º
        
        Returns: å‡†ç¡®ç‡ (0-1)
        """
        correct = 0
        for pred, gt in zip(completions, self.get_batch()):
            if pred.strip() == gt[1].strip():
                correct += 1
        return correct / len(completions)

# åœ¨ scripts/chat_eval.py ä¸­æ³¨å†Œ
from tasks.mytask import MyTask
TASKS = {
    'mytask': MyTask,
    ...
}
```

### è‡ªå®šä¹‰æ¨¡å‹æ¶æ„

```python
# ä¿®æ”¹ nanochat/gpt.py ä¸­çš„ GPT ç±»

class GPTCustom(GPT):
    def __init__(self, config):
        super().__init__(config)
        
        # æ·»åŠ ä½ çš„ç»„ä»¶
        self.custom_layer = MyCustomLayer(config.n_embd)
    
    def forward(self, idx, kv_cache=None):
        # è°ƒç”¨çˆ¶ç±»å‰å‘ä¼ æ’­
        x, cache = super().forward(idx, kv_cache)
        
        # åº”ç”¨è‡ªå®šä¹‰å±‚
        x = self.custom_layer(x)
        
        return x, cache

# åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨
model = GPTCustom(config)
```

### ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨

```python
# åœ¨ scripts/base_train.py ä¸­ä¿®æ”¹ä¼˜åŒ–å™¨é€‰æ‹©

if optimizer_choice == "adamw":
    param_groups = [
        {'params': embedding_params, 'lr': embedding_lr, 'weight_decay': weight_decay},
        {'params': matrix_params, 'lr': matrix_lr, 'weight_decay': weight_decay},
    ]
    optimizer = torch.optim.AdamW(param_groups)

elif optimizer_choice == "sgd":
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

elif optimizer_choice == "lion":
    # éœ€è¦å®‰è£…: pip install lion-pytorch
    from lion_pytorch import Lion
    optimizer = Lion(model.parameters(), lr=0.001)
```

### é›†æˆè‡ªå®šä¹‰æ•°æ®

```python
# æ–¹å¼ 1: JSON è¡Œæ ¼å¼
echo '{"prompt": "Hello", "completion": " world"}' > data.jsonl
python -m scripts.base_train --data_path=data.jsonl

# æ–¹å¼ 2: ä½¿ç”¨ CustomJSON ä»»åŠ¡
from tasks.customjson import CustomJSON
task = CustomJSON('data.jsonl')

# æ–¹å¼ 3: åˆ›å»ºè‡ªå®šä¹‰ Dataset
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, texts):
        self.texts = texts
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        return self.texts[idx]
```

---

## ğŸ“ å­¦ä¹ èµ„æº

### å¿…è¯»è®ºæ–‡

1. **Attention is All You Need** (2017)
   - Transformer æ¶æ„çš„åŸå§‹è®ºæ–‡
   - å¿…è¯»ï¼šç†è§£æ•´ä¸ªæ¨¡å‹åŸºç¡€

2. **Rotary Position Embeddings** (2021)
   - RoPE ä½ç½®ç¼–ç 
   - ç†è§£ NanoChat çš„ä½ç½®ç¼–ç 

3. **Grouped Query Attention** (2023)
   - GQA æ¨ç†ä¼˜åŒ–
   - ç†è§£ KV ç¼“å­˜èŠ‚çœ

4. **Training Compute-Optimal LLMs** (Chinchilla, 2022)
   - å‚æ•°å’Œæ•°æ®çš„æœ€ä¼˜ç¼©æ”¾
   - ç†è§£ä¸ºä»€ä¹ˆ d20 éœ€è¦ 20B tokens

### é¡¹ç›®ä»£ç å¯¼èˆª

**ä»è¿™é‡Œå¼€å§‹ï¼š**
```
README.md                    â† é¡¹ç›®æ¦‚è¿°
speedrun.sh                  â† å®Œæ•´è®­ç»ƒæµç¨‹
â”œâ†’ PROJECT_MAP_CN.md         â† é¡¹ç›®åœ°å›¾ï¼ˆæœ¬æ–‡æ¡£ï¼‰
â””â†’ ADVANCED_PRINCIPLES_CN.md â† æ·±åº¦åŸç†
```

**é€ä¸ªå­¦ä¹ æ ¸å¿ƒæ¨¡å—ï¼š**
```
1. nanochat/tokenizer.py     â† åˆ†è¯ç³»ç»Ÿ
2. nanochat/gpt.py           â† æ¨¡å‹æ¶æ„
3. nanochat/engine.py        â† æ¨ç†å¼•æ“
4. scripts/base_train.py     â† è®­ç»ƒå¾ªç¯
5. nanochat/dataloader.py    â† æ•°æ®åŠ è½½
```

---

## ğŸ› å¸¸è§é—®é¢˜ (FAQ)

**Q: æˆ‘åªæœ‰ 1 ä¸ª GPU (A100-40GB)ï¼Œèƒ½è®­ç»ƒå—ï¼Ÿ**

A: å¯ä»¥ï¼Œä½†éœ€è¦å¤§å¹…é™ä½é…ç½®ï¼š
```bash
python -m scripts.base_train \
  --depth=10 \
  --device_batch_size=4 \
  --total_batch_size=4096 \
  --num_iterations=1000
```
è®­ç»ƒä¼šå¾ˆæ…¢ï¼ˆ~20 å€ï¼‰ï¼Œä½† code paths æ˜¯ä¸€æ ·çš„ã€‚

**Q: å¦‚ä½•æ¢å¤ä¸­æ–­çš„è®­ç»ƒï¼Ÿ**

A: NanoChat è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼š
```bash
# ç»§ç»­ä»æœ€æ–°æ£€æŸ¥ç‚¹
torchrun --standalone --nproc_per_node=8 \
  -m scripts.base_train \
  --resume=latest

# æŒ‡å®šç‰¹å®šæ­¥éª¤
torchrun --standalone --nproc_per_node=8 \
  -m scripts.base_train \
  --resume_step=10000
```

**Q: å¦‚ä½•è‡ªå®šä¹‰æ¨¡å‹çš„"ä¸ªæ€§"ï¼Ÿ**

A: ç¼–è¾‘ä¸­é—´è®­ç»ƒæ•°æ®ï¼š
```bash
# ä¸‹è½½å¹¶ä¿®æ”¹èº«ä»½å¯¹è¯
curl -o identity.jsonl https://...
# ç¼–è¾‘ identity.jsonl æ¥æ”¹å˜é£æ ¼

# æ··åˆåˆ°è®­ç»ƒæ•°æ®
python -m scripts.mid_train \
  --identity_data=identity.jsonl
```

**Q: æ¨¡å‹ç”Ÿæˆçš„è´¨é‡å¾ˆå·®ï¼Œæ€ä¹ˆåŠï¼Ÿ**

A: å°è¯•è¿™äº›ä¼˜åŒ–ï¼š
1. å¢åŠ é¢„è®­ç»ƒæ•°æ®ï¼š`python -m nanochat.dataset -n 500`
2. å¢åŠ æ¨¡å‹å¤§å°ï¼š`--depth=26`
3. å¢åŠ  SFT æ•°æ®é‡å’Œè¿­ä»£
4. è°ƒæ•´é‡‡æ ·å‚æ•°ï¼š`--temperature=0.7 --top_p=0.9`

**Q: èƒ½ç”¨ CPU/MPS è¿è¡Œå—ï¼Ÿ**

A: å¯ä»¥ï¼Œå‚è€ƒ `dev/runcpu.sh`ï¼š
```bash
python -m scripts.base_train \
  --device_type=cpu \
  --depth=4 \
  --device_batch_size=1 \
  --max_seq_len=512 \
  --num_iterations=20
```

---

## ğŸ“ è·å–å¸®åŠ©

- **GitHub Issues**: é¡¹ç›®é—®é¢˜è¿½è¸ª
- **Discussions**: ä¸€èˆ¬è®¨è®ºå’Œé—®é¢˜
- **DeepWiki**: ä»£ç é—®ç­”ï¼ˆdeepwiki.com/karpathy/nanochatï¼‰

---

**ç¥ä½ æˆåŠŸè®­ç»ƒå±äºè‡ªå·±çš„ LLMï¼** ğŸš€
